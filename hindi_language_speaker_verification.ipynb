{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XqcwpndSt8E",
    "outputId": "e204d927-200b-4dd0-e3cd-6e5523fde801"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-28cb41438edc>:13: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  if torchaudio.get_audio_backend() != 'sox_io':\n",
      "<ipython-input-2-28cb41438edc>:14: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Tokenizer, HubertModel\n",
    "# Check if torchaudio's sox_io backend is available\n",
    "if torchaudio.get_audio_backend() != 'sox_io':\n",
    "    torchaudio.set_audio_backend(\"sox_io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yVh0ezJl5CRD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PQkxDq-28H3h"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_path = []\n",
    "directory = \"datasets/test_data/audio\"\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    data_path.append(filename)\n",
    "df = pd.DataFrame({\"filename\":data_path})\n",
    "df['user_id'] = df['filename'].apply(lambda x: '_'.join(x[:-4].split('-')[1:]))\n",
    "\n",
    "df1 = df.drop_duplicates(subset='user_id', keep='first').reset_index(drop=True)\n",
    "df2 = df.drop_duplicates(subset='user_id', keep='last').reset_index(drop=True)\n",
    "\n",
    "final_l1 = pd.DataFrame()\n",
    "final_l2 = pd.DataFrame()\n",
    "\n",
    "final_l1['audio_1'] = df1.sort_values(by='user_id',ascending=True)['filename'].values\n",
    "final_l1['audio_2'] = df2.sort_values(by='user_id',ascending=True)['filename'].values\n",
    "final_l1['level'] = 1\n",
    "\n",
    "final_l2['audio_1'] = df1.sort_values(by='user_id',ascending=True)['filename'].values\n",
    "final_l2['audio_2'] = df2.sort_values(by='user_id',ascending=False)['filename'].values\n",
    "final_l2['level'] = 0\n",
    "\n",
    "final_test = pd.concat([final_l1,final_l2],axis=0).reset_index(drop=True)\n",
    "\n",
    "# Write DataFrame to a text file\n",
    "final_test.to_csv('data_test.txt', sep='\\t', index=False, header=False, line_terminator='\\n')\n",
    "\n",
    "# Step 1: Define a dataset class to load the data\n",
    "class data_load(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            self.data = [line.split() for line in file.readlines()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define your model architecture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Load pre-trained model and tokenizer\n",
    "        model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "        #self.model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\").eval()\n",
    "        self.model = Wav2Vec2Model.from_pretrained(model_name).eval()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Assuming x1 and x2 are paths to audio files\n",
    "        # You need to implement how to load and process audio files into features\n",
    "        # Convert audio files to features\n",
    "        feature1 = self.preprocess_audio(x1)\n",
    "        feature2 = self.preprocess_audio(x2)\n",
    "\n",
    "        # Extract embeddings\n",
    "        with torch.no_grad():\n",
    "            out1 = self.model(feature1.unsqueeze(0)).last_hidden_state\n",
    "            out2 = self.model(feature2.unsqueeze(0)).last_hidden_state\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        out1_emb = out1.squeeze(0)\n",
    "        out2_emb = out2.squeeze(0)\n",
    "\n",
    "        # Ensure the dimensions match\n",
    "        min_length = min(out1_emb.shape[0], out2_emb.shape[0])\n",
    "        output1 = out1_emb[:min_length]\n",
    "        output2 = out2_emb[:min_length]\n",
    "\n",
    "\n",
    "        # Here, you need to define how to compute similarity between output1 and output2\n",
    "        # For example, you can use cosine similarity, Euclidean distance, etc.\n",
    "        similarity_score = self.compute_similarity(output1, output2)\n",
    "\n",
    "        return similarity_score\n",
    "\n",
    "\n",
    "    # Function to preprocess audio clips\n",
    "    def preprocess_audio(self, audio_path):\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        TARGET_SAMPLE_RATE =16000\n",
    "        # Resample if necessary\n",
    "        if sample_rate != TARGET_SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # # Convert stereo to mono if necessary\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        #print(waveform.shape)\n",
    "        # Ensure single channel\n",
    "        waveform = waveform.squeeze(0)  # Remove batch dimension if present\n",
    "        #print(waveform.shape)\n",
    "        if waveform.dim() > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)  # Take the mean if multiple channels\n",
    "\n",
    "        # Normalize waveform\n",
    "        waveform /= torch.max(torch.abs(waveform))\n",
    "\n",
    "        return waveform\n",
    "\n",
    "\n",
    "    def compute_similarity(self, output1, output2):\n",
    "        # Implement similarity computation here\n",
    "        # For demonstration, let's assume we compute cosine similarity between output1 and output2\n",
    "        # You might need to reshape or process the outputs before computing similarity\n",
    "        # Here's a simple example of computing cosine similarity\n",
    "        # Note: This is just a placeholder. Implement the actual similarity computation as needed.\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(output1, output2, dim=1)\n",
    "        return similarity.mean().item()\n",
    "\n",
    "# Step 3: Calculate EER\n",
    "def calculate_eer(scores, labels):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer   # Convert to percentage\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = data_load(\"data_test.txt\")\n",
    "\n",
    "# Initialize the model\n",
    "model = MyModel()\n",
    "\n",
    "# Load pre-trained weights if necessary\n",
    "# model.load_state_dict(torch.load('pretrained_model.pth'))\n",
    "\n",
    "# Lists to store scores and labels\n",
    "scores = []\n",
    "labels = []\n",
    "no_file = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VHiSTqbiDENg"
   },
   "outputs": [],
   "source": [
    "# Iterate through the dataset and generate scores\n",
    "for data in dataset:\n",
    "    # Here, data[0] and data[1] represent the paths of two audio files for comparison\n",
    "    # You need to implement how you load and process these audio files\n",
    "    # Then, pass them through your model to get the similarity score\n",
    "    #try:\n",
    "    score = model('datasets/test_data/audio/'+data[0], 'datasets/test_data/audio/'+data[1])  # Adjust this line according to your model's input\n",
    "\n",
    "    scores.append(score)\n",
    "    labels.append(int(data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIdpU9QgrZCv",
    "outputId": "93de8b29-5646-49af-b1cc-a0086f214869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate EER for Wav2Vec2Model\n",
    "eer = calculate_eer(scores, labels)\n",
    "print(f\"EER: {eer:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yu5vGQ_mcNZ0",
    "outputId": "80a06e00-0b04-4633-d6de-dd394b42821f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.45%\n"
     ]
    }
   ],
   "source": [
    "# Calculate EER for HubertModel\n",
    "eer = calculate_eer(scores, labels)\n",
    "print(f\"EER: {eer:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT8El5khos3-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
